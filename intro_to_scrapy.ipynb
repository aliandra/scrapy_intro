{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://scrapinghub.files.wordpress.com/2016/01/scrapylogo.png\" />\n",
    "\n",
    "# Introduction to Scrapy\n",
    "\n",
    "Scrapy is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival.\n",
    "\n",
    "Scrapy runs from the command line.\n",
    "\n",
    "[Scrapy Docs](https://doc.scrapy.org/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following in the command line to install Scrapy package:\n",
    "```\n",
    "$ pip install scrapy\n",
    "```\n",
    "<br>\n",
    "\n",
    "**Linux (Ubuntu 12.04 or above)**\n",
    "```\n",
    "$ sudo apt-get install python-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev\n",
    "\n",
    "$ sudo apt-get install python3 python3-dev  #if running python3 only\n",
    "\n",
    "$ pip install scrapy\n",
    "```\n",
    "<br>\n",
    "\n",
    "**Mac OS X**   (If pip install fails)  \n",
    "[installation guide](https://doc.scrapy.org/en/latest/intro/install.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the command line, navigate to the directory where you want to save your project and run:\n",
    "```\n",
    "$ scrapy startproject tutorial\n",
    "```\n",
    "```tutorial``` is the name of this project. This will create a ```tutorial``` directory with all the files and folders Scrapy needs to run. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spiders are a class that you define which Scrapy uses to scrape information from a website (or a group of websites).\n",
    "\n",
    "In the ```tutorial``` directory, you will find a directory called ```spiders```. Here is where we will save our scrapy script.\n",
    "\n",
    "The 'bones' of scrapy spider should look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class ExampleSpider(scrapy.Spider):\n",
    "    name = 'example'\n",
    "\n",
    "    custom_settings = {\n",
    "        \"DOWNLOAD_DELAY\": 3,\n",
    "        \"CONCURENT_REQUESTS_PER_DOMAIN\": 3,\n",
    "        \"HTTPCASHE_ENABLED\": True\n",
    "    }\n",
    "\n",
    "    start_urls = ['http://www.example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # script goes here!        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spider 'ExampleSpider' subclasses scrapy.Spider and defines some attributes and methods:\n",
    "\n",
    "- name: identifies the Spider. It must be unique within a project.\n",
    "\n",
    "- custom_settings: Here you can set custome settings for your spider.\n",
    "\n",
    "- start_url: Spider will begin to crawl from this url.\n",
    "\n",
    "- parse(): a method that will be called to handle the response downloaded for each of the start_urls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**TIP: BE NICE TO WEBSITES**:  \n",
    "Always make sure that your crawler follows the rules defined in the website’s robots.txt file. This file is usually available at the root of a website (www.example.com/robots.txt) and it describes what a crawler should or shouldn’t crawl according to the Robots Exclusion Standard. Hitting a website too hard might harm it, slow it down for others, and might even get your IP banned!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selectors: XPath v CSS\n",
    "Selectors are patterns we can use to find one or more elements on a page so we can then work with the data within the element. Scrapy supports both CSS and XPath selectors. With XPath, you can extract data based on text elements’ contents, and not only on the page structure. So when you are scraping the web and you run into a hard-to-scrape website, XPath may be your best option and save lots of time. \n",
    "\n",
    "For additional help with XPath, check out [this tutorial](https://blog.scrapinghub.com/2016/10/27/an-introduction-to-xpath-with-examples/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy Shell\n",
    "The best way to test out your code! The scrapy shell is an interactive environment to test selectors on a specific website and see what they extract. This will help you debug your spiders before you run them. In the command line run:\n",
    "\n",
    "```\n",
    "$ scrapy shell 'https://www.example.com'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Your First Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Music Fesitval Scraper\n",
    "We are going to scrape information on music festivals listed on Music Festival Wizard. Specifically, we want to scrape information on all music festivals in the [United States](https://www.musicfestivalwizard.com/festival-guide/us-festivals/) and [Canada](https://www.musicfestivalwizard.com/festival-guide/canada-festivals/). Take a look at both the US and Canada festival lists and notice that they have a similar structure. \n",
    "\n",
    "We want our spider to crawl through both lists and follow the links to each individual festival listing where we can then scrape out the information we want.  \n",
    "\n",
    "Let's save the following script in a file called ```festival.py``` in the ```tutorial/tutorial/spiders``` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class FestivalSpider(scrapy.Spider):\n",
    "    name = 'music_festivals'\n",
    "\n",
    "    custom_settings = {\n",
    "        \"DOWNLOAD_DELAY\": 3,\n",
    "        \"CONCURRENT_REQUESTS_PER_DOMAIN\": 3,\n",
    "        \"HTTPCACHE_ENABLED\": True\n",
    "    }\n",
    "\n",
    "    start_urls = [\n",
    "        'https://www.musicfestivalwizard.com/festival-guide/us-festivals/',\n",
    "        'https://www.musicfestivalwizard.com/festival-guide/canada-festivals/'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extract the links to the individual festival pages\n",
    "\n",
    "\n",
    "        # Follow pagination links and repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Scrapy Shell to write and test the Spider. In the command line run:\n",
    "```\n",
    "$ scrapy shell 'https://www.musicfestivalwizard.com/festival-guide/us-festivals/'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grabbing Links  \n",
    "This first thing we want our Spider to do is grab the links for the individual festivals so we can go to each of those pages and extract some data. \n",
    "\n",
    "If we inspect the first link we see that address for the webpage we want is embeded in an < a > tag. Inspecting the next link shows a simlar tagging. Notice the < a > tags are inside of < span class = 'festivaltitle' >\n",
    "\n",
    "In the Scrapy Shell try:\n",
    "\n",
    "```\n",
    "In [1]: response.xpath('//span[@class=\"festivaltitle\"]/a/@href').extract()\n",
    "```\n",
    "<br>\n",
    "*Breakdown*\n",
    "- response: an object representing the webpage content that is downloaded by Scrapy\n",
    "- xpath: the selector we are using the select elements from the page. We include the directions to the elements we want in the parenthesis. \n",
    "- extract(): a method to extract elements. There is also an 'extract_first()' method.\n",
    "\n",
    "<br>\n",
    "Notice that the above returns all the links to the individual festival pages on the current page. We can now write a for loop to follow each link and scrape that page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def parse(self, response):\n",
    "        # Extract the links to the individual festival pages\n",
    "        for href in response.xpath(\n",
    "                '//span[@class=\"festivaltitle\"]/a/@href'\n",
    "        ).extract():\n",
    "            # For each festival link, call 'parse_festival' (defined later)\n",
    "            yield scrapy.Request(\n",
    "                url=href,\n",
    "                callback=self.parse_festival,\n",
    "                meta={'url':href}\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parse method looks for the individual festival links and then yeilds a new request for each of those links, calling the parse_festival method which we will define later. The meta parameter is essentially a dictionary that is copied to the parse_festival method. \n",
    "\n",
    "This only gets the links on page 1 of this list. What if we want to copy the links for all the pages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next page\n",
    "After we've looped through all the links on the current page, we want to be able to go to the next page and do the same. Let's inspect the arrow to the next page and get the link. Using the Scrapy Shell try to figure out the Xpath to this link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def parse(self, response):\n",
    "        # Extract the links to the individual festival pages\n",
    "        for href in response.xpath(\n",
    "                '//span[@class=\"festivaltitle\"]/a/@href'\n",
    "        ).extract():\n",
    "            # For each festival link, call 'parse_festival' (which we will define)\n",
    "            yield scrapy.Request(\n",
    "                url=href,\n",
    "                callback=self.parse_festival,\n",
    "                meta={'url':href}\n",
    "            )\n",
    "\n",
    "        # Follow pagination links and repeat\n",
    "        next_url = response.xpath(\n",
    "            '//div[@class=\"pagination\"]/ul/li/a[@class=\"next page-numbers\"]\\\n",
    "            /@href').extract()[0]\n",
    "\n",
    "        yield scrapy.Request(\n",
    "            url=next_url,\n",
    "            callback=self.parse\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the link to the next page, all we need to go is call parse on that link and the cycle will repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets extract some data for each festival!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to define parse_festival to scrape the data we want to get from each of the festival listing. We want to scrape the following:\n",
    "- Festival Name\n",
    "- Location\n",
    "- Dates\n",
    "- Ticket price\n",
    "- Festival website\n",
    "- Logo url\n",
    "- Lineup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**TIP: XPATH IN BROWSERS**:  \n",
    "When inspecting the HTML of a website, right click in the html code on the item you want and copy the XPath information.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Festival Name\n",
    "The XPath for the festival name looks like this:\n",
    "\n",
    "//*[@id=\"post-25879\"]/div/header/h1/span\n",
    "\n",
    "We want to extract the text between the span tags. In the Scrapy shell, lets try the following:\n",
    "\n",
    "```\n",
    "In [1]: response.xpath('//h1/span/text()').extract()\n",
    "```\n",
    "The Output we get is \n",
    "```\n",
    "Out [1]: [u'Panorama Music Festival 2017']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Festival Name\n",
    "name = response.xpath('//h1/span/text()').extract()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Location\n",
    "Next lets look at location. The XPath is:\n",
    "//*[@id=\"festival-basics\"]/text()[1]\n",
    "\n",
    "Lets look in the scrapy shell and see what the following returns:\n",
    "```\n",
    "In [2]: response.xpath('//div[@id=\"festival-basics\"]/text()').extract()\n",
    "\n",
    "Out[2]:\n",
    "[u'\\r\\n',\n",
    " u'\\r\\n',\n",
    " u'\\r\\n',\n",
    " u'New York City, NY',\n",
    " u'\\r\\n',\n",
    " u'July 28-July 30, 2017',\n",
    " u'\\r\\n',\n",
    " u' ',\n",
    " u'\\r\\n',\n",
    " u' No',\n",
    " u'\\r\\n',\n",
    " u'\\r\\n\\r\\n',\n",
    " u'\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n',\n",
    " u'\\r\\n',\n",
    " u'\\r\\n\\r\\n',\n",
    " u'\\r\\n\\r\\n',\n",
    " u'\\r\\n\\r\\n\\r\\n',\n",
    " u'\\r\\n\\r\\n']\n",
    "```\n",
    "\n",
    "We get a list with the location at index 3 and dates at index 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location = (\n",
    "    response.xpath('//div[@id=\"festival-basics\"]/text()').extract()[3])\n",
    "\n",
    "dates = (\n",
    "    response.xpath('//div[@id=\"festival-basics\"]/text()').extract()[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ticket Price\n",
    "Try this one on your own- Make sure to use the Scrapy Shell!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Website\n",
    "Let's look at the xpath for the festival's official website:\n",
    "//*[@id=\"festival-basics\"]/a\n",
    "\n",
    "Notice that the web address is inside the 'a' tag and is assigned to href. To extract an href link we use:  \n",
    "```\n",
    "In [3]: response.xpath('//div[@id=\"festival-basics\"]/a/@href').extract()[0]\n",
    "\n",
    "Out[3]: u'http://www.panorama.nyc/'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logo and Lineup\n",
    "Try to do these on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's put it all together in parse_festival :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_festival(self, response):\n",
    "        url = response.request.meta['url']\n",
    "\n",
    "        name = response.xpath('//h1/span/text()').extract()[0]\n",
    "\n",
    "        location = (\n",
    "            response.xpath('//div[@id=\"festival-basics\"]/text()').extract()[3])\n",
    "\n",
    "        dates = (\n",
    "            response.xpath('//div[@id=\"festival-basics\"]/text()').extract()[5])\n",
    "\n",
    "        tickets = (\n",
    "            response.xpath('//div[@id=\"festival-basics\"]/text()').extract()[7])\n",
    "\n",
    "        website = (\n",
    "            response.xpath(\n",
    "                '//div[@id=\"festival-basics\"]/a/@href').extract()[0]\n",
    "        )\n",
    "\n",
    "        logo = (\n",
    "            response.xpath(\n",
    "                '//div[@id=\"festival-basics\"]/img/@src').extract()[0]\n",
    "        )\n",
    "\n",
    "        lineup = (\n",
    "            response.xpath(\n",
    "                '//div[@class=\"lineupguide\"]/ul/li/text()').extract() +\n",
    "            response.xpath(\n",
    "                '//div[@class=\"lineupguide\"]/ul/li/a/text()').extract()\n",
    "        )\n",
    "\n",
    "        yield {\n",
    "            'url': url,\n",
    "            'name': name,\n",
    "            'location': location,\n",
    "            'dates': dates,\n",
    "            'tickets': tickets,\n",
    "            'website': website,\n",
    "            'logo': logo,\n",
    "            'lineup': lineup\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we added the 'url' variable from meta which we defined earlier in the parse method. Putting it all together our ```festival.py``` script should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class FestivalSpider(scrapy.Spider):\n",
    "\n",
    "    name = 'music_festivals'\n",
    "\n",
    "    custom_settings = {\n",
    "        \"DOWNLOAD_DELAY\": 3,\n",
    "        \"CONCURRENT_REQUESTS_PER_DOMAIN\": 3,\n",
    "        \"HTTPCACHE_ENABLED\": True\n",
    "    }\n",
    "\n",
    "    start_urls = [\n",
    "        'https://www.musicfestivalwizard.com/festival-guide/us-festivals/',\n",
    "        'https://www.musicfestivalwizard.com/festival-guide/canada-festivals/'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        for href in response.xpath(\n",
    "            '//span[@class=\"festivaltitle\"]/a/@href'\n",
    "        ).extract():\n",
    "\n",
    "            yield scrapy.Request(\n",
    "                url=href,\n",
    "                callback=self.parse_festival,\n",
    "                meta={'url': href}\n",
    "            )\n",
    "\n",
    "        next_url = response.xpath(\n",
    "            '//div[@class=\"pagination\"]/ul/li/a[@class=\"next page-numbers\"]\\\n",
    "            /@href'\n",
    "        ).extract()[0]\n",
    "\n",
    "        yield scrapy.Request(\n",
    "            url=next_url,\n",
    "            callback=self.parse\n",
    "        )\n",
    "\n",
    "    def parse_festival(self, response):\n",
    "\n",
    "        url = response.request.meta['url']\n",
    "\n",
    "        name = response.xpath('//h1/span/text()').extract()[0]\n",
    "\n",
    "        location = (\n",
    "            response.xpath('//div[@id=\"festival-basics\"]/text()').extract()[3])\n",
    "\n",
    "        dates = (\n",
    "            response.xpath('//div[@id=\"festival-basics\"]/text()').extract()[5])\n",
    "\n",
    "        tickets = (\n",
    "            response.xpath('//div[@id=\"festival-basics\"]/text()').extract()[7])\n",
    "\n",
    "        website = (\n",
    "            response.xpath(\n",
    "                '//div[@id=\"festival-basics\"]/a/@href').extract()[0]\n",
    "        )\n",
    "\n",
    "        logo = (\n",
    "            response.xpath(\n",
    "                '//div[@id=\"festival-basics\"]/img/@src').extract()[0]\n",
    "        )\n",
    "\n",
    "        lineup = (\n",
    "            response.xpath(\n",
    "                '//div[@class=\"lineupguide\"]/ul/li/text()').extract() +\n",
    "            response.xpath(\n",
    "                '//div[@class=\"lineupguide\"]/ul/li/a/text()').extract()\n",
    "        )\n",
    "\n",
    "        yield {\n",
    "            'url': url,\n",
    "            'name': name,\n",
    "            'location': location,\n",
    "            'dates': dates,\n",
    "            'tickets': tickets,\n",
    "            'website': website,\n",
    "            'logo': logo,\n",
    "            'lineup': lineup}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Your Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run your spider, in the command line, go to the project's top level directory and run:\n",
    "\n",
    "```python\n",
    "scrapy crawl music_festivals -o festival.json\n",
    "```\n",
    "\n",
    "This will run out spider named 'music_festivals' and generate a 'festivals.json' file containing all scraped data in json. Scrapy can also save data in other formats such as a csv file. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:snakes]",
   "language": "python",
   "name": "conda-env-snakes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
